\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{datetime2}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[a4paper, left=1in, right=1in, bottom=1.25in]{geometry}

% set up headers and footers
\pagestyle{fancy}
\fancyhf{}
\lhead{Operating System Concepts Notes}
\rfoot{Page \thepage}
\setlength{\headheight}{15pt}

% highlights links
% apparently need to import hyperref last
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=cyan,
}

\begin{document}
\section*{OS Concepts 1.1: What Operating Systems Do}

\begin{itemize}
    \item \textbf{What is an OS?:} OS's vary widely in design and in function, but basically an OS is the software that sits between application programs and computer hardware. It provides an environment in which application programs are executed, by allocating physical resources like CPU, memory, and I/O devices.
\end{itemize}

\section*{OS Concepts 1.2: Computer-System Organization}

\subsection*{1.2.1: Interrupts}

\begin{itemize}
    \item \textbf{Device Controller:} The I/O managing processor within a device.
    \item \textbf{Device Driver:} A component in the OS that understands how to communicate with its respective device controller and manages I/O to those devices.
    \item \textbf{Interrupts:} Interrupts are used in OS's to handle asynchronous events originating from outside the processor (interrupts originating from within the processor are called exceptions). Device controllers and hardware faults raise interrupts. Because interrupts are used so heavily for time-sensitive processing, efficient interrupt handling is necessary for good system performance.
    \item \textbf{Interrupt Vector:} A table of pointers stored in low memory that holds the addresses of the interrupt service routines.
    \item \textbf{Basic Interrupt Implementation:} The CPU hardware has a wire called the interrupt-request line that the CPU senses after executing every instruction. When the CPU detects a device controller has asserted a signal on the wire, it reads the interrupt number and jumps to the respective interrupt-handler routine by using the interrupt number as an index into the interrupt vector. It then saves the current state of whatever was interrupted, and starts execution of the interrupt-handler routine. Once the handler is finished executing, it performs a state restore and returns the CPU to the execution state prior to the interrupt.
    \item \textbf{Interrupt Terminology:} We say that the device controller \textbf{raises} an interrupt by asserting a signal on the interrupt request line, the CPU \textbf{catches} the interrupt and \textbf{dispatches} it to the interrupt handler, and the handler \textbf{clears} the interrupt by servicing the device.
    \item \textbf{More Sophisticated Interrupt Implementation:} We need the ability for the following:
        \begin{itemize}
            \item Defer interrupt handling during critical processing
            \item Efficiently dispatch to the correct interrupt-handler without having to first poll all devices to see which one raised the interrupt
            \item Multilevel interrupts, so that the OS can distinguish between high and low priority interrupts and respond with the appropriate level of urgency
            \item A way for an instruction to get the OS's attention directly (separately from I.O requests), for activities such as page faults and errors such as division by zero. This task is accomplished by "traps".
        \end{itemize}
        To do this, most CPU's have two interrupt request lines: one is the nonmaskable interrupt, which is used for events such as unrecoverable memory errors, and the second is the maskable interrupt, which the CPU can turn off before the execution of critical instruction sequences that must not be interrupted. Device controllers use the maskable interrupt to request service.
\end{itemize}

\subsection*{1.2.2: Storage Structure}

\begin{itemize}
    \item \textbf{Firmware:} Software stored in ROM or EEPROM for booting the system and managing low level hardware.
\end{itemize}

\subsection*{1.2.3: I/O Structure}

\begin{itemize}
    \item \textbf{Direct Memory Access (DMA):} Interrupt-driven I/O as described in section 1.2.1 is fine for moving small amounts of data but can produce high overhead when used for bulk data movement, like when moving data to and from nonvolatile memory. DMA is used to avoid this overhead. The device controller sets up buffers, pointers, and counters for its I/O device, and transfers entire blocks of data to or from the device and main memory, with no intervention by the CPU. Only one interrupt is generated per block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated for low-speed devices. The CPU is able to perform other work while the device controller is performing these operations.
\end{itemize}

\section*{OS Concepts 1.3: Computer-System Architecture}

\subsection*{1.3.1: Single-Processor Systems}

\begin{itemize}
    \item \textbf{CPU:} The hardware that executes instructions.
    \item \textbf{Processor:} A physical chip that contains one or more CPU's.
    \item \textbf{CPU Core:} The core is the component of the CPU that executes instructions and contains registers for storing data locally.
    \item \textbf{Single-Processor System:} A computer system with a single processor containing one CPU with a single processing core. These systems often also have other special-purpose processors as well, such as disk, keyboard, and graphics controllers. These special-purpose processors run a limited instruction set and do not run processes; their use is incredibly common and does not turn a single-processor system into a multiprocessor system.
\end{itemize}

\subsection*{1.3.2: Multiprocessor Systems}

\begin{itemize}
    \item \textbf{Multiprocessor Systems:} A computer system containing multiple processors (figure \ref{fig:symmetric-multiprocessing-architecture}). Traditionally contains two or more processors, each with a single-core CPU.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.6\textwidth]{figures/symmetric-multiprocessing-architecture.jpg}
            \caption{Symmetric \textit{multiprocessing} architecture}
            \label{fig:symmetric-multiprocessing-architecture}
        \end{figure}
    \item \textbf{Multiprocessor Advantages (Increased Throughput):} Primary advantages of multiprocessor systems is increased throughput. The speed-up ratio with \(N\) processors is not \(N\), however; it is less than \(N\) because there is overhead incurred and contention for shared resources when dealing with multiple processors.
    \item \textbf{Multicore Systems:} A computer system containing multiple cores on the same processor chip (figure \ref{fig:multicore-architecture}). Such systems can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication. Additionally, one chip with multiple cores uses significantly less power than multiple single-core chips, an issue especially important for mobile devices.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.6\textwidth]{figures/multicore-architecture.jpg}
            \caption{\textit{Multicore} architecture}
            \label{fig:multicore-architecture}
        \end{figure}
    \item \textbf{Multiprocessor Bottleneck:} Adding additional CPU's to a multiprocessor system increases computing power, but does not scale very well. Once too many CPU's are added, contention for the system bus becomes a bottleneck and performance begins to degrade.
    \item \textbf{Non-uniform Memory Access (NUMA):} To avoid bottleneck performance degradation arising from system bus contention, we can provide each CPU with its own local memory that is accessed via a small and fast local bus (figure \ref{fig:numa-architecture}). The CPU's are connected by a shared system interconnect, so that all CPU's share one physical address space. The advantage is that when a CPU accesses its local memory, not only is it fast, but there is also no contention over the system interconnect. Thus, NUMA systems can scale more effectively as more processors are added.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.4\textwidth]{figures/numa-architecture.jpg}
            \caption{NUMA architecture}
            \label{fig:numa-architecture}
        \end{figure}
    \item \textbf{NUMA Drawbacks (Increased Latency):} A potential drawback is increased latency when a CPU must access remote memory across the system interconnect (accessing the local memory of another CPU). OS's can minimize this NUMA penalty through careful CPU scheduling and memory management.
\end{itemize}

\section*{OS Concepts 1.4: Operating-System Operations}

\begin{itemize}
    \item \textbf{Bootstrap Program:} The initial program that is run when a computer starts running for the first time. Typically a very simple program and stored in firmware. The program must know how to load the OS kernel into memory and start executing it.
    \item \textbf{System Daemons:} Services provided outside of the kernel that are loaded into memory at boot time, which run the entire time the kernel is running.
\end{itemize}

\subsection*{1.4.1: Multiprogramming and Multitasking}

\begin{itemize}
    \item \textbf{Multiprogramming:} Increase CPU utilization by organizing programs so that the CPU always has one to execute. Execute a process until it needs to wait on some task like I/O, then switch to another process. Keep switching between processes such that the CPU is never idle.
    \item \textbf{Process:} In a multiprogrammed system, a program in execution is termed a process.
    \item \textbf{Multitasking:} The logical extension of multiprogramming. In multitasking systems, the CPU executes multiple processes by switching among them incredibly frequently. This is how interactive I/O like keyboard input works: rather than letting the CPU sit idle in the time between the keystrokes of the user, the OS will rapidly switch to another process in the meantime.
\end{itemize}

\subsection*{1.4.2: Dual-Mode and Multimode Operation}

\begin{itemize}
    \item \textbf{Modes of Execution:} A properly designed OS must ensure that an incorrect (or malicious) program cannot cause other programs - or the OS itself - to execute incorrectly. To do this, we need to distinguish between the execution of OS code and user-defined code. Most computer systems provide hardware support that allows differentiation among various modes of execution.
    \item \textbf{User Mode and Kernel Mode:} At the very least, we need two separate modes of operation: user mode and kernel mode (also called supervisor mode, system mode, or privileged mode). A bit, called the mode bit, is added to the hardware of the computer to indicate the current mode: kernel (0) or user (1). Whenever the OS gains control of the computer, it is in kernel mode. The system always switches back to user mode before passing control to a user program. The concept of modes can be extended beyond two modes (e.g. the four protection rings in Intel processors).
    \item \textbf{Privileged Instructions:} Some machine instructions that may cause can only be executed in kernel mode. The instruction to switch to kernel mode is an example of a privileged instruction.
\end{itemize}

\subsection*{1.4.3: Timer}

\begin{itemize}
    \item \textbf{Timer:} We must ensure that the OS maintains control over the CPU: we allow user programs to execute, but they must eventually relinquish control to the OS (avoid situations like user program infinite loops or not calling system services and thus not returning control to the OS). To accomplish this, we can use a timer that is set to raise an interrupt after a specified amount of time. If the timer interrupts, control transfers automatically to the OS, which may treat the interrupt as a fatal error or may give the program more time. Instructions that modify the timer are clearly privileged.
\end{itemize}

\section*{OS Concepts 1.5: Resource Management}

The OS can be seen as a resource manager. The following are things that the OS must carefully manage in a computer system.

\subsection*{1.5.1: Process Management}

\begin{itemize}
    \item \textbf{Program vs Process:} A program by itself is not a process. A program is a \textit{passive} entity, whereas a process is an \textit{active} entity. Remember that a process is just a program in execution, thus there can be multiple processes associated with the same program (and each is considered a separate execution sequence).
\end{itemize}

\subsection*{1.5.2: Memory Management}

\begin{itemize}
    \item \textbf{Memory Management:} The OS is responsible for keeping tack of which parts of memory are currently being used and which process is using them, allocating and deallocating memory, and deciding which processes (or parts of processes) and data to move into and out of memory.
\end{itemize}

\subsection*{1.5.3: File-System Management}

\begin{itemize}
    \item \textbf{File System:} The OS abstracts the physical properties of its storage devices to define a logical storage unit, the \textbf{file}. In other words, the OS implements the abstract concept of a file by managing mass storage media and the devices that control them.
\end{itemize}

\subsection*{1.5.4: Mass-Storage Management}

\begin{itemize}
    \item \textbf{Secondary Storage Management:} The proper management of secondary storage is critical to a computer system. The OS must take care of things such as mounting and unmounting, free-space management, storage allocation, disk scheduling, partitioning, and protection.
\end{itemize}

\subsection*{1.5.5: Cache Management}

\begin{itemize}
    \item \textbf{OS and Memory Hierarchy:} The OS is responsible for moving data between the different levels of the memory hierarchy that it has access to. The OS can only manipulate software-controlled caches, for instance transfer of data from disk to memory, while data transfer from CPU cache to registers is a hardware function.
    \item \textbf{Caches and Multitasking:} In a computing environment where only one process executes at a time, having the same data appear in multiple levels of the memory hierarchy is not an issue, since access to desired memory always will be to the copy at the highest level of the hierarchy. In a multitasking environment, however, extreme care must be taken to ensure that if several processes wish to access the same data, each of these processes obtains the most recently updated value of the data.
    \item \textbf{Caches and Multiprocessor Systems:} In a multiprocessing environment, not only do we need to make sure that processes access the most recenly updated value of the desired data (multitaking), but we now have CPUs that contain local caches in which data may esist simultaneously in several of these. We need to make sure that an update to the value of a given piece of data in one cache is immediately reflected in all other caches where this data resides. This issue if called \textit{cache conherency}, and is usually handled in hardware (below the OS level).
\end{itemize}

\subsection*{1.5.6: I/O System Management}

\begin{itemize}
    \item \textbf{I/O Subsystem:} One of the purposes of an OS is to hide the peculiarities of specific hardware devices from the user. Often, these peculiarities are hidden from most of the OS itself by the I/O subsystem. Device drivers for specific hardware devices for instance are included in the I/O subsystem.
\end{itemize}

\section*{OS Concepts 1.7: Virtualization}

\begin{itemize}
    \item \textbf{Virtualization:} Virtualization allows us to abstract the hardware of a single computer (the CPU, memory, disk drives, etc.) into several different execution environments, creating the illusion that each separate environment is running on its own private computer. An OS that is natively compiled for a particular CPU architecture runs within another OS also native to that CPU.
    \item \textbf{Emulation:} Simulates computer hardware in software, typically used when the source CPU type is different from the target CPU type (e.g. Apple Rosetta when moving from PowerPC to x86). Usually much slower than native code.
\end{itemize}

\section*{OS Concepts 1.10: Computing Environments}

\subsection*{1.10.4: Peer-to-Peer Computing}

\begin{itemize}
    \item \textbf{Peer-to-Peer (P2P) Computing:} In this distributed computing model, clients and servers are not distinguished from each other. Each node in the system may act as either a client or a server, depending on whether it is requesting or providing a service. In traditional client-server systems, the server is a bottleneck; but in a P2P system, services can be provided by several nodes distributed throughout the network.
    \item \textbf{Centralized P2P:} When a node first joins a network, it registers its service with a centralized lookup service on the network. Any node desiring a specific service first contacts this centralized lookup service to determine which node provides the service. The remainder of the communication takes place between the client and the service provider.
    \item \textbf{Decentralized P2P:} A decentralized system uses no centralized lookup service. Instead, a peer acting as a client must discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network. To support this approach, a \textit{discovery protocol} must be provided that allows peers to discover services provided by other peers in the network.
\end{itemize}

\section*{OS Concepts 2.6: Why Applications are OS Specific}

\begin{itemize}
    \item \textbf{Why Applications are OS Specific:} Each OS exposes different functionalities (system calls), so applications cannot expect to be able to use the same functions across varying OS's. Even if system calls were somehow uniform, other barriers would still pose a challenge: binary formats, varying CPU ISA's, system call discrepencies (specific operands and operand ordering, how to invoke syscalls, syscall result meanings, etc.).
    \item \textbf{How to Make Applications Cross Compatible Across OS's:}
    \begin{itemize}
        \item Write the application in an \textit{interpreted language} (e.g. Python or Ruby); performance typically suffers. and interpreter usually only offers a subset of the OS's features.
        \item Write the application in a language that includes a \textit{virtual machine} (e.g. Java). The JVM has been ported to many OS's, and in theory any Java app can run within the JVM wherever it's available. Usually have similar disadvantages as with interpreted languages.
        \item Use a language that compiles \textit{machine and OS specific binaries} (e.g. C++ or Rust), and simply port to each OS on which it will run. Standard API's like POSIX can make this process easier.
    \end{itemize}
\end{itemize}

\section*{OS Concepts 2.7: OS Design and Implementation}

\subsection*{2.7.1: Design Goals}

\begin{itemize}
    \item \textbf{OS Goals and Specifications:} The first problem in designing a system is to define goals and specifications. These requirements can, however, be divided into two basic groups: user goals and system goals.
\end{itemize}

\subsection*{2.7.2: Mechanisms and Policies}

\begin{itemize}
    \item \textbf{Mechanisms and Policies:} An important principle is the separation of policy from mechanism. Mechanisms determine \textit{how} to do something; policies determine \textit{what} will be done. For instance, the standard Linux kernel has a specific CPU scheduling algorithm, which is a mechanism that supports a certain policy. However, anyone is free to modify or replace the scheduler to support a different policy.
\end{itemize}

\section*{OS Concepts 2.8: OS Structure}

\subsection*{2.8.1: Monolithic Structure}

\begin{itemize}
    \item \textbf{Monolithic Structure:} A monolithic kernel places all of the functionality of the kernel into a single, static binary file that runs in a single address space. Everything below the system-call interface and above the physical hardware is the kernel, as seen in figure \ref{fig:monolithic-structure}. Typically difficult to implement and extend, but have good performance due to very little overhead in the syscall interface, and communication within the kernel is fast.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.7\textwidth]{figures/monolithic-structure.jpg}
            \caption{Traditional UNIX system structure (monolithic architecture)}
            \label{fig:monolithic-structure}
        \end{figure}
\end{itemize}

\subsection*{2.8.3: Microkernels}

\begin{itemize}
    \item \textbf{What is a Microkernel:} The microkernel approach structures the OS by removing all nonessential components from the kernel and implementing them as user-level programs that reside in separate address spaces, resulting in a smaller kernel, seen in figure \ref{fig:microkernel-structure}. There is little consensus on what services remain in the kernel, however, typically minimal process and memory management and a communication facility are provided. The main function of the microkernel is to provide communication between the client program and the various services also running in user space; communication is provided though message passing.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{figures/microkernel-structure.jpg}
            \caption{Architecture of a typical microkernel}
            \label{fig:microkernel-structure}
        \end{figure}
    \item \textbf{Microkernel Benefits:} The microkernel approach makes extending the OS easier, as all new services are added to user space and do not require modification of the kernel. When the kernel does have to be modified, the changes tend to be fewer as the kernel is smaller. It also makes porting the OS to different hardware easier and provides more security and reliability, as most services are running as user -rather than kernel- processes.
    \item \textbf{Microkernel Drawbacks:} Performance of microkernels can suffer due to increased system-function overhead. The overhead involved in copying messages and switching between processes has been the larged impediment to the groth of microkernel-based OS's.
\end{itemize}

\subsection*{2.8.4: Modules}

\begin{itemize}
    \item \textbf{Loadable Kernel Modules (LKMs):} Using LKMs, the kernel has a set of core components and can link in additional services via modules, either at boot time or during run time. The key idea is for the kernel to provide core services, while other services are implemented dynamically, as the kernel is running.
\end{itemize}

\section*{OS Concepts 2.9: Building and Booting an OS}

\subsection*{2.9.2: System Boot}

\begin{itemize}
    \item \textbf{Booting OS:} When starting a computer system, how does the hardware know where the kernel is or how to load that kernel? This process of loading the kernel is known as booting the system. The boot process typically roughly follows as so:
    \begin{enumerate}
        \item A small piece of code known as the bootstrap program or boot loader locates the kernel
        \item The kernel is loaded into memory and started
        \item The kernel initializes the hardware
        \item The root file system is mounted
    \end{enumerate}
    \item \textbf{BIOS:} Some (typically older) computers use a multistage boot process: when the computer first powered on, a small boot loader located in nonvolatile firmware known as BIOS is run. This initial bootloader usually does nothing more than load a second boot lader, which is located at a fixed disk location called the boot block, which is then responsible for loading the OS into memory and begin execution.
    \item \textbf{UEFI:} More recent computers have replaced the BIOS-based boot process with UEFI (Unified Extensible Firmware Interface). The biggest difference is that UEFI is a single, complete boot manager and therefore is faster than the multistage BIOS boot process.
\end{itemize}

\section*{OS Concepts 3.1: Process Concept}

\subsection*{OS Concepts 3.1.1: The Process}

\begin{itemize}
    \item \textbf{What is a process?} A process is a program in execution. A program becomes a process when an executable file is loaded into memory. Although two processes may be associated with the same program, they are nevertheless considered two separate esecution sequences.
    \item \textbf{How is the status of a running process represented?} The status of the current activity of a process is represented by the value of the program counter and the contents of the processor's registers (the preservation of a processes memory address space depends on the memory management method of the OS).
    \item \textbf{What does the memory layout of a process look like?} The memory layout of a process is typically divided into multiple sections (figure \ref{fig:process-memory-layout}), the most important being:
        \begin{itemize}
            \item \textbf{Text} - the executable code
            \item \textbf{Data section} - global variables
            \item \textbf{Heap section} - memory that is dynamically allocated during program runtime
            \item \textbf{Stack section} - temporary data storage when invoking functions (such as function parameters, return addresses, and local variables)
        \end{itemize}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.4\textwidth]{figures/process-memory-layout.jpg}
            \caption{Memory layout of a process}
            \label{fig:process-memory-layout}
        \end{figure}
\end{itemize}

\subsection*{OS Concepts 3.1.2: Process State}

\begin{itemize}
    \item \textbf{What are the different states a process can be in?} In general, a process may be in one of the following states (diagram in figure \ref{fig:process-states}):
    \begin{itemize}
        \item \textbf{New:} The process is being created.
        \item \textbf{Running:} Instructions are being executed.
        \item \textbf{Waiting:} The process is waiting for some event to occur (such as an I/O completion).
        \item \textbf{Ready:} The process is waiting to be assigned to a processor.
        \item \textbf{Terminated:} The process has finished execution.
    \end{itemize}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/process-states.jpg}
        \caption{Diagram of process states}
        \label{fig:process-states}
    \end{figure}
\item \textbf{How many processes can be running on a processor core at any given time?} Only one process can be actively running (in the running state) on any processor core at any given time. Many processes may be in the ready or waiting state, however.
\end{itemize}

\subsection*{OS Concepts 3.1.3: Process Control Block}

\begin{itemize}
    \item \textbf{How is information about a process tracked by the OS?} The OS maintains information about each process in a data structure called the process control block (PCB), shown in figure \ref{fig:process-control-block}. Basically it contains all the necessary information required to start, or restart, a process, along with bookkeeping data. On systems that support threads, the PCB is expanded to include information for each thread.
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.4\textwidth]{figures/process-control-block.jpg}
        \caption{The process control block. The highlighted blue regions is the machine environment during the time the process is actively in control of the CPU.}
        \label{fig:process-control-block}
    \end{figure}
\end{itemize}

\section*{OS Concepts 3.2: Process Scheduling}

\begin{itemize}
    \item \textbf{How are processes selected to be run?} To maximize CPU utilization by having some process running at all times and to support efficient multitasking by switching among processes frequently enough, the \textbf{process scheduler} is responsible for selecting an available process for program execution on a CPU core (balancing multiprogramming and time sharing). For a single core system there will never be more than one process running at a time, whereas a multicore system can run multiple processes at a time.
    \item \textbf{Degree of multiprogramming:} The number of processes currently in memory.
    \item \textbf{I/O Bound:} An I/O bound process is one whose speed is bound by the I/O (i.e. it doesn't matter if the CPU is blazing fast, the program speed will still be limited by how fast I/O completes).
    \item \textbf{CPU Bound:} A CPU bound process is one whose speed is limited by the speed of the CPU (i.e. it would go faster if the CPU could go faster).
\end{itemize}

\subsection*{OS Concepts 3.2.1: Scheduling Queues}

\begin{itemize}
    \item \textbf{What happens to a process when it first enters the system?} As processes enter the system, they are put into a \textbf{ready queue}, where they have the "ready" state and waiting to be executed (dispatched) on a CPU's core. This queue is typically implemented as a linked list, containg pointers to PCB structures. See figure \ref{fig:queueing-diagram}.
    \item \textbf{What happesn to a process when it is waiting?} A process in a waiting state (waiting for I/O, time slice expired, etc.) is put into the \textbf{wait queue}. See figure \ref{fig:queueing-diagram}.
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/queueing-diagram.jpg}
        \caption{Representation of process scheduling}
        \label{fig:queueing-diagram}
    \end{figure}
\end{itemize}

\subsection*{OS Concepts 3.2.3: Context Switch}

\begin{itemize}
    \item \textbf{What happens when the CPU needs to switch to a different process?} When switching to another process, whether that be because of an interrupt, time slice expired, or any another reason, the current context of the running process on the CPU core needs to be saved (state save). The context is represented in the PCB of the context. The CPU then needs to restore the context of the other process it wishes to switch to (state restore). This process is known as a \textbf{context switch}.
    \item \textbf{What are the performance implications of a context switch?} A context switch is pure overhead. The system does no useful work while switching processes. A typical context switch takes several microseconds, but is highly dependent on the hardware the OS is running on.
\end{itemize}

\section*{OS Concepts 3.3: Operations on Processes}

\subsection*{OS Concepts 3.3.1: Process Creation}

\begin{itemize}
    \item \textbf{How are processes created?} During the course of execution, a process may create several new processes. The creating process is called the \textbf{parent} process, and the new processes it has created are aclled the \textbf{children} of that process. Each of these new processes may in turn create other processes, forming a hierarchical tree.
    \item \textbf{How are processes identified?} Most OS's use unique process identifiers (PID, typically an integer) to identify processes. The PID provides a unique value for each process in the system.
    \item \textbf{What is the ultimate parent process (chicken and egg problem)?} In a Linux OS, the \texttt{systemd} process (which always has a PID of 1) serves as the root process for all user processes. It is the first user process created when the system boots. Once the system has booted, the \texttt{systemd} process creates processes that provide additional services, like an \texttt{ssh} server or a login server.
    \item \textbf{How are resources and initialization of processes handled?} In general, child processes require resources (CPU time, memory resources, I/O devices, files) to accomplish its task. There are several options how this is handled:
        \begin{itemize}
            \item Parent and child share all resources
            \item Children share subset of parent's resources
            \item Parent and child share no resources
        \end{itemize}
    \item \textbf{How are parent and child processes executed?} The parent process may continue to execute \textbf{concurrently} with its children, or it may \textbf{wait} until some or all of its children have terminated.
    \item \textbf{What are the address-space possibilities for the new child process?} The child process is a \textbf{duplicate} of the parent process (it has the same program and data as input), or the child process has a program \textbf{loaded} into it.
    \item \textbf{What does process creation on a UNIX system typically look like?} Diagram in figure \ref{fig:unix-process-creation}. The parent process calls \texttt{fork()}, at which both the parent and child process continue execution (of the same program). After a \texttt{fork()} call, one of the processes typicall uses the \texttt{exec()} system call to replace the process's memory space with a new program. The parent can then create more children; or, if it has nothing else to do while the child runs, it can issue a \texttt{wait()} system call to move itself off the ready queue until the termination of the child.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{figures/unix-process-creation.jpg}
            \caption{Representation of typical process creation on a UNIX system}
            \label{fig:unix-process-creation}
        \end{figure}
\end{itemize}

\subsection*{OS Concepts 3.3.2: Process Termination}

\begin{itemize}
    \item \textbf{What happens when a process is terminated?} When a process is terminated, all the resources of the process (including virtual and physical memory, open files, and I/O buffers) are deallocated and reclaimed by the OS.
    \item \textbf{Under what situations can a process be terminated?} A process can be terminated under three circumstances:
        \begin{itemize}
            \item Process executes last statement and asks the OS to delete it (\texttt{exit()} syscall, either called explicitly or implicitly by C run-time library)
            \item Process performs an illegal operation (e.g. access memory it is not authorized for or execute a privileged instruction)
            \item Parent may terminate child process (child process exceeding resource usage, task assigned to child no longer required, or parent is exiting and OS uses cascading termination - if a process terminates so must all its children)
        \end{itemize}
\end{itemize}

\section*{OS Concepts 3.4: Interprocess Communication}

\begin{itemize}
    \item \textbf{How do processes communicate with each other?} Using interprocess communication (IPC): implemented using either shared memory or message passing.
\end{itemize}

\section*{OS Concepts 3.5: IPC in Shared-Memory Systems}

\begin{itemize}
    \item \textbf{How does shared memory IPC work?} Normally, the OS tries to prevent processes from accessing memory of other processes. Shared memory IPC requires that two or more processes agree to remove this restriction, so that they can then exchange information by reading and writing data in the shared memory areas.
    \item \textbf{How is shared memory access coordinated among processes?} Processes must themselves ensure that they are not writing to the same location simultaneously.
\end{itemize}

\section*{OS Concepts 3.6: IPC in Message-Passing Systems}

\begin{itemize}
    \item \textbf{What is message passing IPC?} Message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space (particularly useful in distributed environments). Message passing may be either \textbf{blocking} or \textbf{nonblocking} (also known as synchronous or asynchronous).
\end{itemize}

\section*{OS Concepts 3.7: Examples of IPC Systems}

\subsection*{OS Concepts 3.7.4: Pipes}

\subsubsection*{OS Concepts 3.7.4.1: Ordinary Pipes}

\begin{itemize}
    \item \textbf{How do ordinary pipes work for IPC?} Ordinary (in contrast to named) pipes allow two processes to coomunicate in standard producer-consumer fashion: the producer writes to the write end of the pipe, and the consumer reads from the read end of the pipe. As a result, ordinary pipes are unidirectional (one-way communication), and bidirectional communication requires two pipes.
    \item \textbf{What processes can access an ordinary pipe?} On UNIX systems, ordinary pipes cannot be accessed from outside the process that created it. Typically, a parent process creates a pipe and uses it to communicate with a child process it created using \texttt{fork()}. In UNIX systems a child process inherits open files from its parent, hence why it is allowed to access the pipe. This also implies that ordinary pipes can only be used for communication between processes on the same machine.
\end{itemize}

\subsubsection*{OS Concepts 3.7.4.2: Named Pipes}

\begin{itemize}
    \item \textbf{What is difference between ordinary and named pipes?} Named pipes provide bidrectional communication, and do not require a parent-child process relationship. Once a named pipe is established, sefveral processes can use it for communication. Named pipes also continue to exist after communicating processes have finished, and must explicitly be deleted.
\end{itemize}

\section*{OS Concepts 4.1: Threads Overview}

\begin{itemize}
    \item \textbf{What are threads?} A thread is a basic unit of CPU utilization. Processes can have multiple threads of control to perform more than one task at a time (threads exist as subsets of a process).
    \item \textbf{What is a thread comprised of?} A thread is comprised of a thread ID, a program counter (PC), a register set. and a stack.
    \item \textbf{What parts of a process do threads share?} Threads technically share everything belonging to a process, but each thread has it's own independent stack. Figure \ref{fig:thread-memory} shows a rough representation of how stacks use memory in the context of a process.
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\textwidth]{figures/thread-memory.jpg}
            \caption{Rough representation of process memory layout with single thread vs multiple threads}
            \label{fig:thread-memory}
        \end{figure}
    \item \textbf{What is the high level difference between threads and processes?} Processes help organize two independent concepts in computing: resource grouping and execution. Resources (program text, data, open files, child processes, etc.) are grouped into a single process so that they can be managed easily. The other concept of a process is a thread of execution. Threads have program counters keeping track of which instructions to execute next, registers to hold its current working variables, and its own stack. Threads operate in the context of processes. Processes are used to group resources together, while threads are entities scheduled for execution on the CPU.
    \item \textbf{Why might one want to use threads instead of processes?} Process creation is time consuming and resource intensive: it is generally more efficient to use one process that contains multiple threads. It makes particular sense to use threads when multiple tasks need to be performed at the same time using the same resources.
\end{itemize}

\section*{OS Concepts 12.1: I/O Overview}

\begin{itemize}
    \item \textbf{Accomodating Varying I/O Devices using Device Drivers:} Hardware I/O devices are constantly evolving, and so are methods required to communicate with them. To accomodate this ever increasing variety of I/O devices, the kernel of an OS is structured to use device-driver modules. The device drivers present a uniform device-access interface to the I/O subsystem, much as syscalls provide a standard interface between the application and the OS.
\end{itemize}

\section*{OS Concepts 12.2: I/O Hardware}

\begin{itemize}
    \item \textbf{Port:} A device communicates with a computer system via a port.
    \item \textbf{Bus:} If devices share a common set of wires, the connection is called a bus. Uses a rigidly defined protocol that specifies a set of messages that can be sent on the wires. Historically called a data highway. Widely used in computer architecture and vary in signaling methods, speed, throughput, and connection methods.
    \item \textbf{Controller:}  A controller is a collection of electronics that can operate a port, a bus, or a devices.
\end{itemize}

\subsection*{12.2.1: Memory-Mapped I/O}

\begin{itemize}
    \item \textbf{Processor and Device Communication:} A controller has one or more registers for data and control signals. The processor communicates with the controller by reading and writing bit patterns in these registers. I/O device control typically consists of four registers: the status, control, data-in, and data-out registers. Data registers are typically 1 to 4 bytes in size, meaning the processor needs to react quickly otherwise new data may overflow the registers and overwrite the old data. Also means I/O is basically done byte by byte.
    \item \textbf{Memory-Mapped I/O:} With memory-mapped I/O, the controller registers are mapped into the address space of the processor. The CPU executes I/O instructions using standard memory data transfer instructions to read and write the controller registers at their mapped locations in physical memory.
    \item \textbf{Memory-Mapped I/O Example:} Memory-mapped I/O for graphics. A thread sends output to the screen by writing data into the memory-mapped region. The controller then generates the screen image based on the contents of this memory. Writing millions of bytes to the graphics memory is faster than issuing millions of I/O instructions to read and write byte by byte.
\end{itemize}

\subsection*{12.2.2: Polling}

\begin{itemize}
    \item \textbf{Polling:} Polled mode I/O between a host and a controller is basically where the host has to repeatedly loop to check if the device is ready to read or write by continually checking the controller status register. Polling the controller naturally requires CPU cycles (read a device register, logical-and to extract a status bit, and branch depending on status). Polling becomes inefficient when it is attempted repeatedly yet rarely finds a device ready for service, while other useful CPU processing remains undone.
\end{itemize}

\subsection*{12.2.3: Interrupts}

\begin{itemize}
    \item \textbf{Purpose of Interrupts:} Interrupts are used throughout modern OS's to handle asynchronous events and to trap to supervisor-mode (kernel mode) routines.
    \item \textbf{Interrupt Handlers and OS Boot Time:} At boot time, the OS probes the hardware buses to determine what devices are present and installs the corresponding interrupt handlers into the interrupt vector.
    \item \textbf{Software Interrupts (Traps):} To get the attention of the OS, a special instruction called a trap can be executed. This instruction has an operand that identifies the desired kernel service. Library functions to issue syscalls typically implemented using traps.
    \item \textbf{Interrupt-driven I/O vs Polled I/O:} Interrupt-driven I/O is now much more common than polling, with polling being used for high-throughput I/O. Some device drivers use both: interrupts when the I/O rate is low, and switch to polling when the rate increases to the point where polling is faster and more efficient.
\end{itemize}

\subsection*{12.2.4: Direct Memory Access}

\begin{itemize}
    \item \textbf{Direct Memory Access (DMA):} Using the expensive general purpose CPU processor to watch status bits and to feed data into a controller register one byte at a time (programmed I/O, PIO) seems wasteful. We can avoid burdening the main CPU with PIO by offloading some of this work to a special-purpose processor called a DMA controller. Essentially, DMA allows us to bypass the CPU and utilize direct memory to device I/O.
    \item \textbf{DMA High Level Implementation:} Basically, the CPU gives the DMA controller pointers to the source and destination locations and the number of bytes to be transferred. The DMA controller then proceeds to operate the memory bus directly, allowing us to perform I/O without the help of the CPU. When the entire transfer is finished, the DMA controller interrupts the CPU.
    \item \textbf{Cycle Stealing:} When the DMA controller seizes the memory bus, the CPU is momentarily prevented from accessing main memory, although it can still access data items in its caches. Although this cycle stealing can slow down CPU computation, offloading the data transfer work to a DMA controller generally improves the total system performance.
\end{itemize}

\section*{OS Concepts 12.3: Application I/O Interface}

\begin{itemize}
    \item \textbf{Accomodating Varying I/O Devices in an OS:} The wide variety of available devices poses a problem for OS implementors (each device has its own set of capabilities, control-bit definitions, and protocols for interacting with the host). How can the OS be designed so that new devices can be attached to the computer without rewriting the OS? And when devices vary so widely, how can the OS give a convenient, uniform I/O interface to applications? Answer: by abstracting I/O hardware with device drivers.
    \item \textbf{Device Drivers:} Device drivers are kernel modules that internally are custom-tailored to specific devices but that export one of the standard OS I/O interfaces. The purpose of the device driver layer is to hide the differences among device controllers from the I/O subsystem of the kernel. Hardware manufacturers can design new devices to be compatible with existing host controller interfaces (such as SATA), or they can write device drivers for popular OS's. Each OS has its own standards for the device driver interface, so they must be ported for each OS.
    \item \textbf{Device Access Conventions:} I/O devices vary among many dimensions, such as synchronous vs asynchronous, sequential or random access, speed of operation, etc. But for the purpose of application access, many of these differences are hidden by the OS, and the devices are grouped into a few conventional types. The major access conventions include: block I/O, character-stream I/O, memory-mapped file access, and network sockets.
\end{itemize}

\subsection*{12.3.3: Clocks and Timers}

\begin{itemize}
    \item \textbf{Programmable Interval Timer:} Most computers have hardware clocks and timers that provide three basic functions: give the current time, give the elapsed time, and set a timer to trigger operation \(X\) at time \(T\). This hardware is called a programmable interval timer. It can be set to wait a certain amount of time and then generate an interrupt, with the option of generating this interrupt periodically as well. The precision of triggers to generate interrupts is limited by the resolution of the timer, together with the overhead of maintaining virtual clocks.
    \item \textbf{Virtual Clocks:} Used to support more timer requests than the number of timer hardware channels. The kernel implements this simply by having a list of of interrupts scheduled both by the kernel and user requests, sorted in earliest-time-first order. It sets the timer for the earliest time, and when the timer interrupts, the kernel just signals the requester that the timer has gone off and reloads the timer with the next earliest interrupt time.
\end{itemize}

\subsection*{12.3.4: Nonblocking and Asynchronous I/O}

\begin{itemize}
    \item \textbf{Blocking I/O} A blocking call causes the execution of the calling thread to be suspended. Blocking application is easier to write than nonblocking application code.
    \item \textbf{Nonblocking I/O:} Nonblocking calls do not suspend the calling threads execution, like blocking calls do. Instead, it returns quickly, with a return value that indicates how many bytes were transferred. One way an application writer could implement this is with multithreading: some threads can perform blocking system calls, while others continue executing. Some OS's also provide nonblocking syscalls.
    \item \textbf{Asynchronous Calls:} An alternative to nonblocking calls. An asynchronous call returns immediately, without waiting for I/O or whatever computation to complete. The calling thread continues to execute its code. The completion of the task at some future time is then communicated to the thread (either setting some variable in the thread address space, or triggering a signal or software interrupt or a call-back routine that is executed outside the control flow of the thread).
\end{itemize}

\subsection*{12.3.5: Vectored I/O}

\begin{itemize}
    \item \textbf{Vectored I/O:} Allows one syscall to perform multiple I/O operations involving multiple locations. This allows multiple separate buffers to have their contents transferred via one syscall, avoiding context switching and syscall overhead. Some versions also provide atomicity.
\end{itemize}

\section*{OS Concepts 12.4: Kernel I/O Subsystem}

\subsection*{12.4.1: I/O Scheduling}

\begin{itemize}
    \item \textbf{I/O Scheduling:} The order in which applications issue system calls rarely is the best choice. Scheduling can improve overall system performance, can share device access fairly among processes, and can reduce the average waiting time for I/O to complete. OS developers implement scheduling by maintaing a wait queue of requests for each device, rearranging the order of the queue according to a scheduling algorithm.
\end{itemize}

\subsection*{12.4.2: Buffering}

\begin{itemize}
    \item \textbf{Buffering:} Buffering is done for three reasons. One reason is to cope with a speed mismatch between the producer and consumer of a data stream. A second use is to provide adaptations for devices that have different data transfer sizes (especially common in computer networking). A third use is to support copy semantics for application I/O.
    \item \textbf{When Buffering Can't Help:} Buffering is useful for smoothing peaks and troughs of data rate, but it can't help if on average:
        \begin{itemize}
            \item Process demand \(>\) data rate (process will spend time waiting)
            \item Data rate \(>\) capability of the system (buffers will fill and data will spill)
            \item Downside: can introduce jitter (deviation from true periodicity of a presumably periodic signal) which is bad for real-time or multimedia
        \end{itemize}
    \item \textbf{Double Buffering:} Double buffering allows decoupling of the producer of the data from the consumer, thus relaxing timing requirements between them. This works by alternating between two buffers, where the producer is writing into one and the consumer is reading from the other, and making the switch when the producer finishes writing into its buffer.
    \item \textbf{Copy Semantics:} Suppose an application has a buffer of data that it wishes to write to disk by calling the \texttt{write()} syscall. If the application modifies the buffer while the I/O is being performed, is the original or the updated data now being written? Copy semantics ensure that the version of the data is the original, the version at the time of the application syscall. A simple way to implement copy semantics is for the OS to copy application data into a kernel buffer before returning control back to the application. Despite the overhead this copying introduces, copying data from application data space to kernel buffers is common because of the useful copy semantics. Clever use of virtual memory mapping and copy-on-write page protection can also be used to implement the same effect.
\end{itemize}

\subsection*{12.4.4: Spooling}

\begin{itemize}
    \item \textbf{Spooling:} Queue output for a device, such as a printer, that cannot accept interleaved data streams.
\end{itemize}

% \vspace{4mm}

% \noindent Test here test

\end{document}
